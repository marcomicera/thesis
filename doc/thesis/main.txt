Introduction
This Master’s thesis has been written at the Technische Universität Darmstadt, Germany, during
a six-month Erasmus+ exchange and under the supervision of Prof. Patrick Eugster † , M.Sc.
Marcel Blöcher † and Prof. Fulvio Risso ‡ .
Abstract
Nowadays there exist several In-Network Processing (INP) solutions that allow tenants to im-
prove their application performance in terms of different metrics: Daiet [24] inventors claim
to achieve an 86.9%-89.3% traffic reduction by performing data aggregation entirely in the
network data plane. Other solutions like NetChain [13] and IncBricks [17] let programmable
switches store data and process queries to cut end-to-end latency; CloudMirror [16] allows client
applications to specify bandwidth and high availability guarantees.
For the time being, it seems that there is still no valid resource allocation algorithm that
takes into account the presence of a network having a data plane that supports (partially
or completely) INP. This thesis has mainly two goals: (i) model and evaluate an Application
Programming Interface (API) through which applications can ask for INP resources and (ii) dis-
cuss the importance of a scheduler which can reject INP requests and propose their server-only
equivalent when needed (e.g., high switch utilization).
Problem statement
Using INP to scale up data centers and improve their overall performance seems a promising
idea: Daiet [24] inventors claim to achieve an 86.9%-89.3% traffic reduction, hence reducing
workload at the servers; NetChain [13] can process queries entirely in the network data plane,
thus eliminating the query processing at servers and cutting the end-to-end latency to as little
as half of an RTT.
Current data center Resource Managers (RMs) (such as Apache YARN [27], Google Omega
[25]) are not completely network-unaware: for instance, some of them are capable of satisfying
affinity rules. CloudMirror [16] even provides bandwidth guarantees to tenant applications.
Still, current RMs do not consider INP resources. As a consequence, tenant applications cannot
request these kind of services while asking for server resources.
Distributed Systems Programming Group, Technische Universität Darmstadt, Germany
Computer Networks Group, Politecnico di Torino, Italy
31.2.1
Modeling INP resources
One of the two goals of this Master’s thesis consists in investigating how to model INP resources
and how to integrate them in RMs. In order to offer INP services to a tenant application, the
latter should be able to ask for INP resources through an API. To do that, INP resources must
be modeled not only to support currently existing INP solutions such as [24] [13] [17] [10], but
also future ones. It may also be convenient to derive a single model to describe both server and
INP resources.
Classic tenant application requests can often be modeled as a key-value data structure.
CloudMirror [16] requires a Tenant Application Graph (TAG) as an input, which is a directed
graph where each vertex represents an application component and links’ weights represent the
minimum requested bandwidth. One possible model could be based on a TAG, describing
network resources and INP services as vertexes or links. Tenant applications could either use the
same model used within the data center or a simplified one, adding another level of abstraction.
1.2.2
INP-aware Resource Managers
In order for everything to work, a network-aware placement algorithm in the Resource Manager
should be able to consider INP and server resources conjunctly: this brings new challenges in
the field of resource management as there are currently no Resource Managers (RMs) doing
this. One problem that could arise is due to the fact that INP resources are typically very
limited in a data center: chapter 7 will argue the importance of an RM which is flexible enough
to propose alternatives based on the current utilization of INP and server resources, since one
kind of physical resource type can become the bottleneck for the other.
4Chapter 2
Background
This chapter aims to introduce the fundamental technologies used in data centers, without
which In-Network Processing (INP) would not even be possible. This chapter starts with a
general description of how resources are managed in a data center § 2.1 (e.g., Virtual Machines)
and ends with a brief introduction of network techniques and concepts which are strictly related
to Resource Managers.
Resource management in data centers
In a data center, resources of any kind are virtualized in order to achieve higher flexibility,
portability, and availability. Usually, both compute and storage resources are virtualized by
means of Virtual Machines (VMs) and/or containers. Flexibility and portability are both au-
tomatically achieved thanks to this resource virtualization, resulting in software that can be
deployed dynamically, run by multiple platforms and even live migrated; availability is usually
simply achieved by not co-locating VMs and/or containers within a single power domain.
Nowadays, multiple Resource Managers (RMs) are using different approaches to solve dif-
ferent design issues. This section examines these existing RMs while trying to categorize them
based on how they face different scheduling problems.
2.1.1
Glossary
This glossary is intended to list the set of definitions for all the terms used in this thesis. This
is needed to avoid ambiguity since this terminology has never been standardized.
2.1.1.1
Physical architecture
The elementary computational unit in a data center is called node. As mentioned in § 2.1,
nodes are being virtualized by means of VMs and containers. Nodes run on servers, which
are grouped into racks. Servers within a rack are usually connected through a so-called Top of
Rack (ToR) switch. Server racks are then grouped into cells (or pods), that some times may
be special-purpose. Usually, a cell is grouped with few small ones used for testing to form up
a cluster. Most of the RMs manage resources in one cluster. One or more clusters form a data
center, and multiple data centers can form a site.
2.1.1.2
Resources
In this thesis, resources are divided by (i) their level of abstraction and (ii) their type. The
Resources glossary reported at the end of this thesis follows a bottom-up approach, starting
5from those terms belonging to the lowest level of abstraction.
2.1.2
Scheduling architectures
Monolithic. Probably the most simple scheduler architecture out there: a monolithic sched-
uler consists of a single instance scheduler applying the same scheduling algorithm for every
incoming job (so there is no concurrency between resource requesters). A centralized scheduling
logic can support more refined job placement. On the other hand, the absence of parallelism
causes a higher latency with respect to other architectures. Although the single instance could
distinguish among different job types hence treating those differently, its maintenance is not
trivial, due to its single instance (and codebase) nature.
Two-level. This architecture requires computer clusters to be dynamically partitioned in
sub-clusters, each having a dedicated scheduler. A centralized resource allocator determines
which and how many resources should be made available to each scheduler: this is done by
sending offers to schedulers (pessimistic concurrency). Conflicts can simply be avoided by not
offering the same resource to multiple schedulers at the time. The same entity is in charge of
dynamically divide clusters into sub-clusters: this is done to avoid resource fragmentation.
Shared-state. Schedulers are not mapped to sub-clusters like in the previous case. Multi-
ple schedulers have access to the entire cluster and there is no centralized resource allocator
assigning resources to schedulers. In this architecture, schedulers will try to acquire resources
(optimistic concurrency), having not only the possibility of choosing between all the resources
in the cluster but also to ask for those which have been already acquired by another scheduler.
To achieve this, a centralized data structure called cell state maintains the allocation informa-
tion of all resources in the cluster, providing, in fact, a shared-state of it. Schedulers will try
to acquire resources by atomically modifying this cell state, that will be modified only if the
request does not cause any conflict. Each scheduler makes its own resource-allocation decision
on a private copy of the cell state, which is updated every time the scheduler tries to acquire
some resource, no matter what the outcome of the attempt is.
2.1.3
Taxonomy
This section tries to underline the different scheduling design issues that RMs must face by
building a simple short taxonomy, following the guidelines provided by Google in their Omega
[25] paper.
Scheduling work partitioning. The workload can be distributed across schedulers in three
different ways: (i) workload-type unaware load balancing, (ii) workload partitioning to special-
ized clusters and, (iii) a combination of the two.
Interference. Schedulers can concurrently ask for the same resources. In the pessimistic
approach, different resources are offered to different schedulers, making it impossible for them
to compete for the same resource: this, of course, represents a lack of parallelism, since resource
offers are made by a logically centralized entity. Instead, the optimistic approach lets every
scheduler claim the desired resources and only conflicting requests are denied, which of course
introduces an overhead.
6Choice of resources. Schedulers can pick amongst (i) all cluster resources or (ii) a subset of
those. When resources are divided into disjoint sets there will be no concurrency by definition.
Making all resources available to all scheduler will make it easier for schedulers to place jobs
with particularly stringent needs, and it is also useful when the scheduling decision must be
taken based on overall state (e.g., the number of free resources in the cluster).
Preemption. Schedulers can be either allowed to preempt other schedulers’ job assignments
or not. Allowing preemption brings greater flexibility at the cost of interrupting an already-
running job. Since all state-of-the-art RMs support preemption, this design choice will be taken
for granted and hence omitted when comparing different RMs in § 3.1.7.
Allocation granularity. Considering that jobs contain multiple tasks that can be scheduled
on different resources, schedulers can either (i) incrementally schedule tasks as soon as new
resources become free or (ii) schedule a job only when all tasks can be scheduled on the spot.
Not all job types can exploit incremental resource acquisition. This technique can also bring
the system to a deadlock in case there is no back-off mechanism that releases resources once a
job cannot acquire all resources in a reasonable amount of time.
Software Defined Networking (SDN)
Perhaps the most fundamental technology without which INP would not even be possible.
Software Defined Networking (SDN) is a networking paradigm whose main pillar is the
separation of the control and data plane. Network switches become simple forwarding devices
(e.g., OpenFlow [18] switches), while the brain of a Software Defined Network resides in its
logically centralized SDN controller, that exercises direct control over the state in the data
plane elements via an SDN Southbound Interface (SBI). Lots of surveys (e.g., [15] and [14])
often associate SDN with OpenFlow [18], as it is by far the most common SBI.
Network Function Virtualization (NFV)
An SDN-related (but not dependent) concept is Network Function Virtualization (NFV).
The virtualization techniques previously mentioned in § 2.1 (e.g., VMs and containers) can
also be used to virtualize network functions (e.g., firewalls, network monitors, etc.) in order to
obtain all the advantages that come along with virtualization in general, such as better resources
utilization, fault-tolerance, higher flexibility, and so on.
This decouples network services software from any kind of dedicated hardware. Virtual
Network Functions (VNFs) can be run in fact on standard commodity servers, allowing a more
flexible network deployment.
SDN and NFV are complementary, but one does not depend upon the other.
In-Network Processing (INP)
Within this project, data center INP refers to the technique of delegating some parts of com-
putation to programmable switches, hence reducing servers’ workload. Common operations
performed by these devices are data aggregation and key-value storage management.
In contrast to NFV, INP tries to delegate as much of computation to the network, but both
techniques can exploit the advantages of SDN for more flexible network management.
7Chapter 3
Analysis
Chapter 2 introduced how resources can be managed in a data center (§ 2.1) and different
network techniques such as Software Defined Networking (SDN) (§ 2.2), Network Function
Virtualization (NFV) (§ 2.3) and In-Network Processing (INP) (§ 2.4). This chapter aims to
dig into the details of these systems and to extract common patterns between similar INP
solutions to then derive a model capable of fully describing INP resources.
Resource Management Frameworks (RMFs) analysis
After having described the basics behind resource management in § 2.1, it is now time to dig into
the details of existing RMs and categorize them following the taxonomy introduced in § 2.1.3.
3.1.1
Borg [28]
Borg [28] is the first container-management system developed by Google. Jobs are divided in
two groups depending on their workload type (long-running and batch) and they were initially
scheduled by a logically centralized controller called Borgmaster. After Google developed the
shared-state Omega [25] scheduler (analyzed in § 3.1.2), Borg [28] adopted the shared-state
architecture as well.
3.1.1.1
Scheduling
Each cell has its own logically centralized controller called Borgmaster, which is essentially split
into two parts: (i) the scheduling part, consisting in one or more schedulers differentiated by
the workload type they handle and (ii) a management unit in charge of handling client Remote
Procedure Calls (RPCs) and communicating with all other Borg [28] agents. Schedulers mainly
work on tasks rather than jobs. Tasks are scanned in a round-robin fashion and their priority
is also taken into account. Schedulers must first find all available machines for the task to be
scheduled (also considering those currently acquired by a task with lower priority) and then
find the best machine amongst them all. This second part is done by taking into consideration
not only user-specified preferences but also data center global goals like minimizing the number
of preempted tasks and allocating tasks on machines which already have the needed packages
installed in order to reduce the installation time, which usually takes about 80% of the total
start-up latency.
83.1.1.2
Details
Jobs. Jobs in Borg [28] are split into multiple tasks which run within a single cell: tasks
belonging to one job cannot be spread amongst multiple cells in the same cluster. In fact,
Borg [28] only operates on cells. Each job has some properties such as a name, an owner, and
most importantly, constraints about machines (e.g., processor architecture, etc.) which will
determine where its tasks will be scheduled. Tasks instead have resource requirements (e.g.,
CPU cores, RAM, etc.) expressed in terms of quotas (an array of resource quantities). Task
properties can be modified at run-time by the job owner: this is done by pushing a new job
configuration file to Borg [28] and ordering the scheduler to update the involved tasks via a
non-atomic transaction. Most of the workload in Borg [28] run in containers rather than VMs
to avoid the virtualization overhead.
Fairness. Jobs contain a priority field, expressed as an integer. Workload types have different
non-overlapping priority bands. Borg [28] schedulers are preemptive, and in order to contain
the negative effects of preemption cascades, the system does not allow internal preemption for
certain priority bands.
Resource reclamation. Tasks do not fully use their resources for their entire lifespan. This
is why the Borgmaster estimates every few seconds what is the actual amount of resources that
each task needs and reclaims the unused resources to make them available for other tasks. This
is done by periodically contacting Borg [28] agents running on each cluster machine, requesting
for fine-grained resource consumption information. The initial estimated amount of resources
actually needed by a task corresponds with its maximum limit, and it then decreases towards
the actual usage. If the actual resource usage exceeds the estimation, than the latter is rapidly
increased. This technique justifies why Borg [28] inventors have noticed that dedicating clusters
for different workload types is inconvenient. They showed how segregating long-running and
batch jobs in different specialized clusters requires 20% to 30% more machines than having
clusters who run both type of jobs using resource reclamation.
3.1.2
Omega [25]
Omega [25] is a parallel, lock-free and optimistic cluster scheduler by Google. As said in [4],
it was born after Borg [28] with the aim of improving its software engineering. There is no
central resource allocator: all of the resource-allocation decisions take place in the schedulers.
Multiple schedulers were first introduced in Omega [25] and then in Borg [28], making the latter
scheduler not monolithic anymore.
3.1.2.1
Scheduling
Omega [25] makes use of a data structure called cell state, containing information about all
the resource allocation in the cluster. Each cell has a shared copy of this data structure, and
each scheduler is given a private, local, frequently-updated copy of cell state that it uses for
making scheduling decisions. According to optimistic concurrency, once a scheduler makes a
placement decision, it updates the shared copy of cell state with a transaction. Whether or
not the transaction succeeds, a scheduler re-syncs its local copy of cell state afterwards and,
if necessary, re-runs its scheduling algorithm and tries again. Omega [25] supports specialized
schedulers: authors have showed the advantages of a MapReduce [7] specialized scheduler in
[25].
93.1.2.2
Conclusions
Since schedulers do not have access to all cluster resources, Mesos [11] cannot support preemp-
tion across different sub-clusters and it cannot apply policies that make use of the complete
cluster state.
3.1.3
Apache TM Hadoop c YARN [27]
Apache TM Hadoop c YARN [27] (for the sake of brevity: Apache TM YARN [27]) is the Resource
Manager of Apache TM Hadoop c , a framework for distributed processing across clusters.
Apache TM Hadoop c was initially an open source implementation of MapReduce [7], but then
the programming model has been separated from the resource management function, resulting
in an application-independent RM known as Apache TM YARN [27].
3.1.3.1
Entities
Each tenant application has an Application Master whose task is to (i) manage the application
life cycle and (ii) negotiate the resources that the application needs with the central RM, making
Apache TM YARN [27] a monolithic scheduler with no interference between tenant applications.
Each node then has a Node Manager thanks to which the RM can allocate tasks on it. The
Node Manager must also periodically monitor resource availability and report failures.
3.1.3.2
Scheduling
Application Masters issue resource request to the RM, containing containers properties and
locality preferences. Upon receiving a resource request, the centralized scheduler generates con-
tainers using available resources periodically advertised by the nodes themselves. The outcome
of this procedure is reported to the Application Master corresponding to the tenant application
who initiated the request. Application Masters are also informed upon inserting new nodes into
the system.
3.1.3.3
Details
Preemption. The RM can also ask to Application Masters to revoke some resources in case
of a shortage. The application will then have a few choices: for instance it can yield containers
that are less important or checkpoint its current status. If an application does not collaborate
with the RM upon receiving a revoking request, the RM will forcibly terminate those targeted
containers.
Failures. The RM represents a single point of failure for the system and its restart causes
the termination of all containers in the cluster, including all their Application Masters. Node
failures are detected by the RM using timeouts (nodes have to periodically contact the RM).
The RM will then inform all Application Masters who are responsible for responsible for the
application life cycle.
3.1.3.4
Conclusions
Undoubtedly, Apache TM YARN [27] dedicates less attention to scalability due to its de facto
monolithic scheduler: there are multiple Application Masters who just take care of the applica-
tion life cycle and do not perform scheduling, which is done instead by a single RM.
10However, Apache TM YARN [27] authors state that the centralized RM can assure fairness,
capacity and locality thanks to the central and global view that it has on the system. They
justify this by pointing out that Apache TM Hadoop c is an open platform which lets different
independent sources share the same cluster, unlike other ”closed-world ” schedulers like Google
Omega [25].
3.1.4
Mesos [11]
Mesos [11] is a two-level cluster scheduler based on resource offers. It has multiple schedulers
since Mesos [11] has been conceived to share clusters between different cluster computing frame-
works since the beginning of its development. By contrast, Apache TM YARN [27] was initially
embedded in the first version of MapReduce [7] and subsequently became independent out of
the necessity to scale Apache TM Hadoop c .
3.1.4.1
Entities
This scheduler has a logically centralized resource allocator in charge of offering resources to
different schedulers. It is called Mesos master and it is replicated for fault tolerance. A scheduler
with its executor (worker) node are together called framework. Nodes running on cluster nodes
are called Mesos slaves.
3.1.4.2
Scheduling
Initially, every cluster node reports to the master node its own available resources. Based on this
data, the master node can then offer resources to application frameworks based on a particular
policy. The master node does not offer the same subset of resources to each scheduler. Obviously
resource conflicts can be avoided by not offering the same resource to multiple schedulers at the
time. Upon receiving resources offers, application frameworks can either reject the offer (in case
it does not satisfy all framework’s constraints) or tell the master which tasks need to be run
on the dedicated resources. Mesos [11] already knows that certain types of frameworks always
reject certain resource offers characterized by some factors, so frameworks can specify filters in
order for the master to automatically avoid proposing certain kind of resources.
The resource allocation logic can be customized, and Mesos [11] includes an allocation mod-
ule based on priority and one based one fairness. Tasks can be preempted, however frameworks
can be offered guaranteed resources on which tasks cannot be preempted.
3.1.5
Guarantee provisioning: CloudMirror [16]
CloudMirror [16] is particularly interesting since it uses a very flexible and descriptive resource
model that will heavily influence the model proposed in § 5.2.2. It also allows client applications
to specify bandwidth and high availability guarantees.
3.1.5.1
Motivation
Prior resource models were not suitable to represent interactive non-batch applications with
very stringent bandwidth requirements. Both the hose and the Virtual Oversubscribed Cluster
(VOC) model are inefficient as they over-allocate bandwidth. The main reason of why this
happens is that both models aggregate bandwidth requirements between different application
components into a single hose: as a consequence, the VM scheduler does not get to know the
actual bandwidth needed between application components. At the opposite extreme there is
the pipe model, which specifies bandwidth guarantees between each and every pair of VMs:
11besides not exploiting statistical multiplexing, it is clearly not scalable. This led CloudMirror
[16] inventors to come up with a new model.
3.1.5.2
Tenant Application Graph
The TAG is a directed graph where each vertex represents an application component and links’
weights represent the minimum requested bandwidth. Each vertex can have an optional size,
denoting the number of VMs belonging to the component.
SB 12
RB 12
Server
comp. 1
SB 23
RB 23
Server
comp. 2
RB 21
SB 21
Server
comp. 3
Server composite
requirements
Figure 3.1: A TAG example
There are two types of edges: (i) self-loop edges, that are equivalent a hose model and
(ii) standard vertex-to-vertex edges. A standard edge from T ier1 to T ier2 is labeled with
an ordered pair of numbers < SB 12 , RB 12 >, indicating respectively the guaranteed bandwidth
with which VMs in T ier1 can send traffic to VMs in T ier2 (SB 12 ) and the guaranteed bandwidth
with which VMs in T ier2 can receive traffic from VMs in T ier1 (RB 12 ).
3.1.5.3
Model advantages
The edge label format < SB, RB > allows the model to exploit statistical multiplexing, since SB
can represent the peak of the sum of VM-to-VM demands instead of the sum of peak demands
needed by the pipe model. Also, since these values represent bandwidth demands between
individual VMs, there is no need to recompute those every time a TAG node (component) sees
its number of VMs change (i.e., scales up/down).
3.1.6
Firmament [9]
Firmament [9] is a centralized cluster scheduler capable of supporting multi-dimensional resource
requirements (§ 3.1.6.2). The scheduler finds an embedding solution by solving a min-cost max-
flow optimization problem over a graph called flow network (§ 3.1.6.1).
3.1.6.1
Flow network
The flow network is a directed weighted graph that contains both physical and logical resources.
Similarly to any other graph-based scheduler (e.g., [20]), a physical resource is connected to a
logical resource anytime the former can be allocated on the latter. In order to reduce the
overall amount of arcs, the flow network makes use of equivalence class aggregates which group
nodes with similar characteristics (e.g., tasks within a job, physical machines with the same
micro-architectural topology, etc.). An aggregate receives arcs from all entities belonging to it.
Physical aggregates can also receive arcs from those logical nodes that can be allocated on any
of the physical entity represented by the aggregate.
12The physical network is faithfully recreated in the graph thanks to aggregates: for instance,
different machines belonging to the same rack are connected to the same rack aggregate, all
racks are connected to the cluster aggregate and so on. The description does not either stop at
the machine level, since also sockets and cores are represented in the graph. Again, inclusion
relationships are expressed in terms of arcs. Special unscheduled aggregates (one for each job)
receive an arc from every task.
The embedding solution is found by running an instance of a min-cost max-flow optimization
algorithm over the network flow. If a task cannot be allocated on any of the physical machines,
its flow will be directed to its corresponding unscheduled aggregate. Arcs’ costs are determined
by a given scheduling policy (or ”cost model”): at the time of writing, Firmament [9] supports
9 different scheduling policies. Intuitively, arcs connecting tasks to unscheduled aggregates will
have an higher cost with respect to the ones ending in a physical entity.
3.1.6.2
Multi-dimensional resource fitting
Firmament’s [9] creator Malte Schwarzkopf introduced the coordinated co-location (CoCo)
cost model in his PhD dissertation - Schwarzkopf, M. (2018). Operating system support for
warehouse-scale computing (Doctoral thesis). https://doi.org/10.17863/CAM.26443.
This scheduling policy has an interesting feature: admission control. Basically a task can
be allocated on a physical machine only when it satisfies all task’s resource requirements (strict
resource fit). If a task cannot be allocated on any of the physical machines, it cannot be even
connected to the cluster aggregate. Given a logical resource in the network flow, CoCo is able
to efficiently determine all subtrees in the physical topology in which it can be allocated. This
is done by storing in each physical aggregate the minimum and maximum amount of available
resources across its children.
3.1.7
RMFs comparison
The table below contains a quick comparison amongst RMs previously analyzed. The taxonomy
introduced in § 2.1.3 has been used for this categorization.
INP solutions
State-of-the-art INP solutions will be discussed in this section with the aim of deriving a model
capable of fully describing INP resources. To that end, it is necessary to dig into the details
and to recognize common patterns between them.
3.2.1
In-network aggregation: Daiet [24]
Daiet [24] is a system that performs in-network data aggregation for partition/aggregate data
center applications (big data analysis such as MapReduce [7], machine learning, graph processing
and stream processing). Instead of letting worker servers entirely perform computation on the
data and then communicate with each other to update shared state or finalize the computation,
the system let network devices perform data aggregation in order to achieve traffic reduction,
thus reducing the processing load at the destination.
The inventors have proven that in-network data aggregation can reduce the network traffic
significantly for machine learning algorithms (e.g., TensorFlow [1]) and for graph analytics
algorithms (e.g., GPS [23]), hence justifying the usefulness of this system. The system has been
designed for P4 [3] and programmable ASICs, and it can be used on any other SDN platform.
13Resource Managers: quick summary
First Borg [28] version
Scheduling architecture
Monolithic
Scheduling work partitioning Specialized clusters
Interference
None
Choice of resources
All cluster resources
Allocation granularity
Incremental
Omega [25] Scheduling architecture
Scheduling work partitioning
Interference
Choice of resources
Allocation granularity Shared-state
Specialized clusters
Optimistic approach
All cluster resources
Per-scheduler policy
Apache TM YARN [27] Scheduling architecture
Scheduling work partitioning
Interference
Choice of resources
Allocation granularity Monolithic
None
No interference
All cluster resources
Incremental
Mesos [11] Scheduling architecture
Scheduling work partitioning
Interference
Choice of resources
Allocation granularity Two-level
Load balancing
Pessimistic approach
Subset of resources
All-or-nothing
Table 3.1: Resource Managers comparison table using the taxonomy introduced in § 2.1.3
3.2.1.1
Details
Controller. When executing a MapReduce program, the job allocator informs the network
controller of the job allocation to the workers. Then, the network controller pushes a set of
rules to network devices in order to (i) establish one aggregation tree for each reducer and
(ii) perform per-tree aggregation. An aggregation tree is a spanning tree from all the mappers
to the reducer.
Packets. Since every reducer has its own aggregation tree associated with it, network devices
should know how to correctly forward traffic according to the corresponding tree: to achieve
this, a special tree ID (that could coincide with the reducer ID) packet field allows network
devices to distinguish different packets belonging to different aggregation trees.
Obviously, they must also know the output port towards the next network device in the tree
and the aggregation function to be performed on the data.
Packets are sent via UDP (therefore communication is not reliable) with a small preamble
that specifies (i) the number of key-value pairs contained in the packet and (ii) the tree ID
whose packet belongs to. They payload is not serialized to achieve a faster computation by
network devices.
14Daiet: basic topology and entities
In-network data aggregation
Aggregated data
Reducer
Job allocation
Aggregated split data
Data
producer
consumer
Input data
allocator
(master)
Job allocation
SDN controller
Flow rules
Aggregator
switch
Key-value pairs
Mapper
Job allocation and split input data
Figure 3.2: Daiet’s [24] basic topology and entities
Daiet: extended
topology
In-network data aggregation
* Mapper
* Mapper
Tree
Aggregator
switch
Aggregator
switch
Reducer
Mapper
Aggregator
switch
Mapper
SDN controller
allocator
(master)
Data
producer
consumer
Figure 3.3: Daiet’s [24] extended topology
3.2.1.2
Algorithm
To store the key-value map, network devices use two hash tables for each tree: one for the keys
and one for the values. Upon a collision, the algorithm checks whether the key is matching
or just the hash is. In the former case, data aggregation is performed. In the latter case, the
conflicting pair will end up in a spillover bucket that will be flushed to the next node as soon as
it becomes full: this is done since this data is more likely to be aggregated by the next network
device if it has spare memory.
A network device will also flush its data as soon as all its children (according to the aggre-
gation tree) have sent their data: this is made possible by forcing network devices to send a
special END packet after transmitting all key-value pairs to their successor.
Used indexes are stored in a index stack to avoid scanning the whole hash table when
flushing.
15Daiet: logical communication pattern
In-network data aggregation
* Mapper
* Mapper
2. Data is partitioned
and aggregated
3. Aggregated data is returned to the client
Aggregator
switch
Aggregator
switch
Reducer
Mapper
**
**
Aggregator
switch
Mapper
SDN controller
allocator
(master)
Data
producer
consumer
1. Client sends data
Figure 3.4: Daiet’s [24] logical communication pattern
Figure 3.5: Messages exchanged during a Daiet [24] instance execution
3.2.1.3
Implementation
The network data plane has been programmed using P4 [3], which brings two main drawbacks:
(i) a match-action table cannot be applied more than once for the same packet, forcing the
programmer to perform loop unrolling in case of multiple headers in the same packet that
need to be modified by the same rule table and (ii) keys must have a fixed size, causing a big
waste of memory in applications where keys have variable-lengths (e.g., strings). The second
drawback will also cause arrays to have fewer but bigger cells, thus increasing the probability
of collisions. If these collisions involve pairs having different keys (section 3.2.1.2), data will not
16be aggregated, causing traffic to increase.
3.2.1.4
Minimum system requirements
For each aggregation tree (i.e., for each reducer), network devices must form a tree whose root
is connected to the reducer and whose leaves are connected to mappers. Each mapper has to
be connected to exactly one network device of the lowest level. Network devices must (i) store
two arrays (one for the keys and one for the values) and (ii) be able to hash keys. The solution
requires that the system has a centralized SDN controller connected to all switches. The SDN
controller must push flow rules to all switches belonging to at least one tree.
3.2.1.5
Conclusions
Besides all the drawbacks brought by P4 [3] listed in section 3.2.1.3, inventors claim to achieve
a 86.9%-89.3% traffic reduction, causing the execution time at the reducer to drop by 83.6% on
average.
3.2.2
Coordination services: NetChain [13]
NetChain [13] is an in-network solution for coordination services, such as distributed locking,
barriers, etc. All these services are realized on top of a strongly-consistent and fault-tolerant
key-value store, which is entirely implemented in the network data plane. The network device
in charge of storing the distributed store is a programmable switch: this brings an obvious
limitation in terms of storage size, that makes NetChain [13] an acceptable solution only when
a small amount of critical data must be stored in the network data plane, e.g., coordination
services.
NetChain [13] can process queries entirely in the network data plane, causing the end-to-end
latency to drop from multiple RTTs to as little as half of one RTT since servers are not involved
in query processing anymore.
3.2.2.1
Details
Packets. Custom UDP packets are used for queries, containing fields like operation, key and
value. Read and write queries only involve the network data plane, while insert and delete
queries involve the network controller to set up entries in switch tables and to perform garbage
collection, respectively.
This is acceptable since coordination services usually perform read and write queries on
already-existing objects, e.g., locks. Each switch has its own IP address, and packet headers
contain the list of addresses of switches to be traversed, allowing those to properly forward
packets to the their successors (from head to tail for write queries and the opposite for read
queries). This list of IP addresses is inserted by the client (a NetChain [13] agent).
Consistency. A variant of Chain Replication [26] is used in the data plane to handle read
and write queries and to ensure strong consistency, while switches reconfiguration is handled
by the network control plane. The main difference with the standard Chain Replication [26]
protocol is that objects are stored on programmable switches instead of servers. Switches are
logically connected together in order to form an oriented chain.
Read queries are processed by the tail switch while write queries are sent to the head switch,
which will forward the updated state to the rest of the chain.
17NetChain: basic topology and entities
In-network solution for coordination services
System reconfigurations
SDN controller
Write requests
Head
switch
Tail
switch
Read requests
Data
producer
consumer
Read/write replies
Rack
Figure 3.6: NetChain’s [13] basic topology and entities
The key-value store is partitioned amongst virtual nodes using consistent hashing, mapping
keys to a hash ring. Each ring segment is stored by f + 1 virtual nodes allocated on different
physical switches, hence tolerating faults involving up to f switches.
3.2.2.2
Implementation
The network data plane has been programmed in P4 [3] while the controller has been coded
in Python, it runs on a server and communicates with switches through the standard Python
RPC library. Switches agents are Python processes who run in the switch OS. Some P4 [3]
drawbacks were already discussed in section 3.2.1.3.
The out-of-order UDP delivery problem is resolved by adding sequence numbers to write
queries, hence serializing those operations, while the loss of packets is coped by client-side retries
based on timeouts.
3.2.2.3
Minimum system requirements
Network devices must form a chain of length f + 1 in order to tolerate f failures. The client
must include the list of IP addresses of all the f + 1 switches to be traversed in each query
packet header (from head to tail for write queries and the opposite for read queries; storing the
entire backward list for read queries is only necessary in case of tail failures).
Network devices must dedicate some local storage to NetChain [13]: more specifically, they
need to store a (i) register array to store values and a (ii) match-action table to store the
keys’ location in the register array and the corresponding action to be performed. The solution
requires that the system has a centralized SDN controller connected to all switches. The SDN
controller must handle switches reconfigurations.
3.2.2.4
Conclusions
NetChain [13] inventors state that the on-chip memory of programmable switches in enough for
coordination services. Assuming a 10 MB partition allocated on each NetChain [13] switch, a
18Figure 3.7: Messages exchanged during a NetChain [13] instance execution
data center with 100 switches can provide a (10 MB·100)/3 = 333 MB storage with a replication
factor of three: that would be enough for the average number of files (22k, from 0 to 1 byte)
managed by a typical Chubby [5] lock service instance, as cited by Google in their corresponding
paper. Likewise, inventors claim that switches total memory is enough for a distributed locking
system: assuming 30 B locks, the previously-mentioned example would be capable of storing
333 MB/30 B = 10M concurrent locks.
3.2.3
In-network caching fabric: IncBricks [17]
IncBricks [17] is a hardware-software co-designed system for in-network caching: it makes use
of network accelerators attached to programmable switches whenever complicated operations
should be performed on payloads. Supporting multiple gigabytes of memory, network acceler-
ators overcome the limited storage problem typical of programmable switches, which usually
have a memory of tens of megabytes.
19NetChain: extended topology
In-network solution for coordination services
SDN controller
Switch chain
Head
switch
Intermediate
switch
Tail
switch
Another
data
producer
consumer
Data
producer
consumer
Rack
Rack
NetChain: logical communication pattern
Figure 3.8:
NetChain’s [13] extended topology
In-network solution for coordination
services
SDN controller
Head
switch
Intermediate
switch
Tail
switch
1. Direct communication either with
the head switch or with the tail switch
Data
producer
consumer
2. Tail switch always reply
Another
data
producer
consumer
Rack
Rack
Figure 3.9: NetChain’s [13] logical communication pattern
3.2.3.1
Details
Hardware. IncBricks [17] is composed by two components: (i) IncBox, an hardware unit
consisting of a network accelerator and a programmable switch, and (ii) IncCache, a software
system for coherent key-value storage. Packets arriving to an IncBox device are first managed
by the switch, which forwards the packet to the network accelerator only if it is labeled as an
in-network cache one.
If there is a match, the programmable switch will check whether the packet has been already
cached by the network accelerator or not, and will forward the packet to the right network
accelerator attached to it in the former case.
20IncBricks: basic topology and entities
In-network caching system
Setting and updating global registration tables
< key, destination, next hop, caching (y/n) >
SDN controller
Switch
Query and
response
Query and
response
Query
issuer
Data
owner
Rack
IncBricks: logical communication pattern
Rack
Figure 3.10: IncBricks’ [17] basic topology and entities
In-network caching system
SDN controller
2. Some switch may
intercept the query
Switch
Switch
Switch
1. Direct communication
with the destination
Query
issuer
Data
owner
Rack
Rack
Figure 3.11: IncBricks’ [17] logical communication pattern
Logic. The system has been designed having a multi-rooted tree topology in mind. For each
key the centralized SDN controller comes up with a set of designated IncBox units allowed to
cache that key. Any other IncBox unit placed between these designated units won’t cache data
with that specific key.
Then, for a given key and a given destination node, the SDN controller establishes a unique
path of designated IncBox units. Every IncBox unit in the system will get to know (i) the set of
immediate designated successors (according to the tree topology) for every key it is responsible
of and (ii) the unique successor used for a given destination and a given key. This data is stored
in the so-called global registration table. Storing the former information can be useful in case
of failures since it is possible to build alternative paths immediately, making the whole system
more reliable. As soon as a failure is detected, the SDN controller updates all the involved
tables.
21Figure 3.12: Messages exchanged during an IncBricks [17] instance execution
3.2.3.2
Implementation
Storage on network accelerators has been implemented as a fixed-size hash table (plus a hash
index table) due to their limited memory space. As mentioned in § 3.2.3.1, IncCache takes care
of the storage system.
A custom coherency protocol keeps track of all data owners in the network: every IncBox stores
a global registration table that maps a destination server for each possible key. This information
is enough since IncBricks [17] ensures that there is a unique path of IncBox units for a given
key. The SDN controller will take care of updating those tables upon failures.
3.2.3.3
Minimum system requirements
Communicating nodes (VMs) represent the leaves of the tree. Each path must include exactly
one root switch. All things considered, it seems reasonable to state that the actual required
topology is a chain starting from a leaf, passing through a root node and ending on another
leaf.
IncBox units must dedicate some local storage to realize the caching system. The solution
requires that the system has a centralized SDN controller connected to all switches. The SDN
controller must set configure network devices in order for them to forward IncBricks [17] packets
accordingly.
22IncBricks: extended topology
In-network caching system
SDN controller
Switch chain
Switch
Switch
Query
issuer
Switch
Data
owner
Rack
Rack
Figure 3.13: IncBricks’ [17] extended topology
3.2.3.4
Conclusions
Authors claim to be able to drop request latency by 30% and to double throughput for 1024
byte values in a small-scale cluster with 24 servers, 2 switches and 4 network accelerators.
3.2.4
Aggregation protocol: SHArP [10]
SHArP [10] stands for Scalable Hierarchical Aggregation Protocol, and it defines a protocol for
reduction operations. This solutions aims to accelerate High Performance Computing (HPC)
applications by offloading some operations to the network. SHArP [10] is targeted to support
the two most used APIs in the HPC area today: MPI [29] and OpenSHMEM [6]. The kind of
operations that can be offloaded to the network are: (i) MPI [29] barrier, reduce and allreduce,
(ii) logic operands like sum, min, max, or, xor, and, (iii) integer and floating-point operations.
3.2.4.1
Details
Entities. Aggregation Nodes (ANs) are logical entities performing reduction operations. Such
a node can either execute on a network device or on a server, and it is implemented as a daemon,
namely sharpd.
All ANs must form an aggregation tree. Multiple trees are allowed in a system. Similarly to
other in-network aggregation solutions like Daiet [24], data is aggregated along the aggregation
tree by ANs, until it reaches a root AN that is in charge of distributing the result.
The protocol also introduces the concept of group, consisting in a subset of physical hosts that
are connected to the tree leaves: for instance, in MPI [29] a group coincides with a communicator.
One aggregation tree supports multiple groups.
Resources are managed by a special management entity called Aggregation Manager (AM).
Faults and errors are always notified to this node, that will also take care of freeing all resources
belonging to the tree in which the error occurred. The detection of faults and errors cannot
be done using timeouts since HPC APIs do not bound the duration of aggregation operations:
this is why faults must be necessarily detected by monitoring.
23SHArP: basic topology and entities
Protocol for reduction operations
Aggregation request
Data
producer
Dedicated
aggregation
node
Aggregated data
Aggregation
node
Data
consumer
Dedicated
local
resources
List of dedicated
SHArP resources
Cluster
resource
manager
Aggregation
manager
Requesting SHArP resources
SHArP: logical communication pattern
Protocol for reduction operations
Figure 3.14: SHArP’s [10] basic topology and entities
1. Group sends data
Data
producer
Aggregation
node
2. Data is aggregated
Aggregation
node
Data
producer Aggregation
node
Cluster
resource
manager Aggregation
manager
3. Aggregated
data is returned
Data
consumer
Data
consumer
Figure 3.15: SHArP’s [10] logical communication pattern
Data flow. Eventually, an host program will need to execute a job on multiple nodes. When
the job is launched and all host processes have been created (e.g., a communicator in MPI [29]),
either the cluster resource manager (like Slurm [31] or IBM R Spectrum LSF) or an MPI [29]
launcher like mpirun will contact the AM, which will dedicate SHArP [10] resources to the job
and return back the list of these allocated resources. At this point, a SHArP [10] group has
been created. Each SHArP [10] daemon sharpd running on every group member will establish
a reliable connection to the dedicated leaf switch. The AM informs the ANs about the switch
resources allocated to the application, not allowing it to exceed the allocation.
Once connections have been established, all group members send an aggregation request
message to their parent AN. Each AN waits for all its children requests before sending the ag-
gregated data piggybacked on another aggregation request to the parent node. ANs temporarily
maintain a data structure to track an aggregation operation’s progress.
As soon as the tree root node receives data from all its children, it performs the final
aggregation and it sends the result to a destination, that could be (i) one or more process
24belonging to one or more groups or (ii) an external process not belonging to any reduction
group. In the former case, the aggregation tree is used to redistribute the result.
3.2.4.2
Implementation
SHArP [10] has been implemented using InfiniBand [19] as communication standard with Mel-
lanox’s SwitchIB-2 TM devices, which provide support for data reduce operations and for barriers.
Nodes in the SHArP [10] tree are InfiniBand [19] end nodes, and links are implemented using
InfiniBand’s [19] Reliable Connections. When distributing a result using a multicast address,
though, an unreliable delivery mechanism is used. One AN can participate to at most 64
different aggregation trees.
3.2.4.3
Minimum system requirements
ANs (usually run by network devices) must form a tree whose leaves are connected to data
producers (a communicator in MPI [29]). Each data producer is connected to only one tree leaf.
The root AN, instead, is connected to the data consumer, which receives the final aggregated
result. ANs must dedicate part of their local memory to the system.
SHArP: extended topology
Protocol for reduction operations
Tree
Data
producer
Aggregation
node
Aggregation
node
Group
Data
producer Aggregation
node
Cluster
resource
manager Aggregation
manager
Data
consumer
Data
consumer
Figure 3.16: SHArP’s [10] extended topology
The special management unit (AM) must act as a SHArP [10] RM, dedicating SHArP [10]
resources to those entities who request for them.
3.2.4.4
Conclusions
For MPI [29] applications SHArP [10] brings a significant advantage in terms of latency: tests
show that the latency improvement factor (latency experienced without SHArP [10] divided by
the one experienced with SHArP [10]) is proportional to the message size. Even in the worst
case, with a message size of just 8 MB, an MPI [29] execution with SHArP [10] is twice as fast
as the one without using it.
25Figure 3.17: Messages exchanged during a SHArP [10] instance execution
Integrating INP resources in Resource Managers (RMs)
There are currently no RMs nowadays that (i) handle logical server, switch and edge resources,
(ii) support high-level requests (e.g., with the use of composites) and (iii) are capable of con-
sidering multiple resource dimensions. However, there are different schedulers that fulfill, even
partially, a subset of those requirements. This paragraph tries to cover all the aspects of INP-
integration from a theoretical point of view.
3.3.1
Interaction with different architectures
Different RM scheduling architectures were introduced in § 2.1.2. This section tries to analyze
how could different scheduling architectures manage INP resources, highlighting all the draw-
backs brought by each one of them. This brief analysis does not make any assumption regarding
the INP resource model since its format is out of the scope of this specific section.
Monolithic. In this architecture, resource requests cannot be concurrent by definition since
there is just one logically centralized scheduler. However, this architecture does not scale due
to its intrinsic nature. Such a scheduling architecture will be used later on to build a greedy
INP-aware scheduler that will underline the general problems that arise when dealing with INP
resources (§ 1.2.2).
Two-level. In the two-level architecture a centralized node master (or allocator ) proposes
resources to the schedulers. Let us suppose that the allocator not only proposes server resources,
but also INP ones: during the period of time which lasts from the instant in which resources are
26offered until the scheduler accepts or denies the offer, all resources must be locked. Locking INP
resources though means locking either network devices’ resources (to perform computation) or
links’ bandwidth (for guarantees), which can heavily affect the performance of all nodes using
these shared network resources.
One could think of not locking INP resources during the delay introduced by the scheduler
when accepting or denying the request, but the allocator would then need to repeat the whole
offering process in case another scheduler accepts some INP resources that have been acquired
by some other scheduler during this period of time.
Shared-state. A shared-state scheduler could include the INP resources state in the cell
state data structure. Supposing that tenant applications can request for both server and INP
resources, schedulers will try to acquire those by modifying the cell state through an atomic
commit. Schedulers that will have to satisfy requests containing supposedly-longer resource
request lists are more likely to cause an higher amount of conflicts when attempting to write
the share cell state data structure. This could heavily affect the whole RM performance.
3.3.2
RMs’ network awareness levels
Existing RMs have a different awareness level of switch and link resources: based on this, RMs
can be divided in three groups.
VMs proximity-aware. Most of the RMs out there can spread VMs across different failure
domains like machines, racks and power domains. This group is not worth discussing since this
kind of RMs do not consider any kind of resources rather than server ones. Some examples of
VMs proximity-aware RMs are Omega [25], Apache TM YARN [27] and Mesos [11].
Bandwidth-aware. Some RMs like CloudMirror [16], Oktopus [2], Kraken [8] and Proteus
[30] allow tenants to specify bandwidth demands. These RMs let tenants express their re-
quests by using ”virtual network” models like Virtual Clusters (VCs), Time-Interleaved Virtual
Clusters (TIVCs), Virtual Oversubscribed Clusters (VOCs) and Tenant Application Graphs
(TAGs).
Oktopus [2] and Kraken [8] assume that every VM can be placed on every physical server
(i.e., VM slots instead of dealing with resource dimensions), completely ignoring server-local
resource requirements. This is not acceptable in a real-world scenario in which different logical
server resources have different resource requirements.
It is also worth to notice that Kraken [8] allows tenants to upgrade their bandwidth require-
ments, causing the migration of VMs that have been previously allocated in parts of the data
center in which the new bandwidth requirements could not be satisfied anymore. Still, none of
these RMs is able to manage any kind of switch resources.
Network resources-aware. The most interesting group consists in those RMs that are actu-
ally aware of network resources. At the time of writing, there seems to be only one embedding
solution that belongs to this group, namely the one introduced in ”On tackling virtual data
center embedding problem” [20] by Rabbani, Md Golam, et al. presented in the IM 2013:
IFIP/IEEE International Symposium on Integrated Network Management conference. This so-
lution allows tenants to explicitly specify (i) logical server resources, (ii) logical switch resources
and (iii) bandwidth demands as logical edge resources.
27Tenants use a graph to express their resource requests. The graph is expressed as a key-value
map that includes (i) a set of VM resources, (ii) a set of logical switch resources and (iii) a set
of logical links connecting the above entities (and their minimum required bandwidth).
Its placement algorithm is interesting for the scope of this thesis since it is the only one
that takes into account all three types of resources mentioned before. The placement phase is
divided in three parts: (i) the VMs placement, (ii) the logical switches placement and (iii) the
logical links placement. The problem of placing VMs is reduced to a min-cost flow one like
showed in Figure 3.18.
Step 1: VMs placement
• The problem is reduced to a min-cost flow problem
VMs sorted according to a
requested resource capacity
in a descending order
Physical servers
VM 1
SRV 1
VM 2
SRV 2
VM 3
SRV M
VM N
Figure 3.18: VM placement in [20]
The graph shown in Figure 3.18 is built in the following way: VMs are sorted according to
a requested resource capacity in a descending order and placed in the left side of the image.
Physical servers are instead placed in the right side. VMs are connected to physical servers
only if they can be allocated on them. A dummy source and destination node are then added
like shown in Figure 3.18 so that an instance of a min-cost flow solver can be run on it. The
outcome of this phase provides the allocation of VMs on physical servers.
It is important to notice how VMs are sorted based on just one resource dimension. Even
though the model used in [20] supports an infinite number of resource dimensions, the placement
algorithm only supports one dimension and it does not seem to exist an easy way to extend it
to support multiple dimensions.
Same thing is done for the placement of logical switches, as shown in Figure 3.19.
The mapping of logical switches to physical ones is done independently of the outcome of
the previous phase (i.e., VM placement). Figure 3.20 shows how this could lead to a bandwidth
waste in case a logical switch connecting two VMs is mapped to a physical switch that is far
away from the physical servers on which the VMs have been previously placed.
The third and last step simply consists in mapping logical links between two logical entities
(VMs or switches) to the shortest physical path connecting the physical devices on which the
logical entities have been allocated.
The placement algorithm is not fault tolerant since it tries to map as many VMs as possible
to the same physical server in order to minimize server resource fragmentation.
28Step 2: virtual switches placement
• Same min-cost flow problem as before
Virtual switches
Physical switches
VS 1
SWC 1
VS 2
SWC 2
VS 3
SWC M
VS K
Figure 3.19: Logical switches placement in [20]
Rack
Allocation request:
Link having enough
residual bandwidth:
Link having insufficient
residual bandwidth:
Links on the
shortest path:
Figure 3.20: An example of an inefficient logical switch placement in [20]
Resource models
Several network abstractions have been proposed to provide simple APIs to tenant applications,
requiring them to only specify high-level resource requests without knowing the actual data
center infrastructure.
The section contains a brief analysis of resource models used by tenants and a short discus-
sion about their ability to describe INP solutions.
3.4.1
Virtual Cluster (VC)
Proposed in the Oktopus [2] paper, a Virtual Cluster (VC) is a logical one-level tree in which
N VMs are connected to a single virtual switch by a bidirectional link of bandwidth B.
29Virtual switch
VM 1
VM N
Server
requirements
Figure 3.21: A graphical representation of the VC model
The virtual switch has a bandwidth of N · B, hence the VC has no oversubscription, unlike
the Virtual Oversubscribed Cluster (VOC) model described in § 3.4.2. Authors say that the
absence of oversubscription makes it suitable for data-intensive applications.
3.4.1.1
Usage
Kraken [8]. The VC is the only resource model used by Kraken [8]. The system allows tenants
to update their minimum guarantees in terms of bandwidth and the amount server resources.
However, tenants can only express the number of needed computing resources (called computing
units) and not their internal requirements (like CPU cores, memory, etc.). The assumption that
all resources are the same is not acceptable in an INP scenario.
Oktopus [2]. Tenant applications can choose one of the three following options to express
their request: they can either (i) stick with the classic resource request by listing individual
server resource demands without expressing bandwidth guarantees, accepting to simply get
some share of the network resources, (ii) choose a VC, most likely used to request resources for
data-intensive applications which do not tolerate oversubscribed networks or (iii) specify a VOC
for those applications having more intra-component communication than an inter-component
one.
Yet, when the Oktopus [2] system receives a request expressed with a VC, it assumes that
VMs can be allocated on any server with enough free VM slots, exactly like Kraken [8] with its
computing units.
3.4.2
Virtual Oversubscribed Cluster (VOC)
Also proposed by Oktopus [2] authors, the Virtual Oversubscribed Cluster (VOC) model consists
in N VMs arranged in groups of size S. Similarly to the VC model, VMs belonging to the same
group are connected to a single virtual switch by a bidirectional link of bandwidth B. Therefore,
every virtual switch connecting groups of S VMs has total bandwidth of N · B. All groups are
then connected together by a unique root virtual switch, making this model’s topology a two-
level logical tree.
Links connecting all virtual switches to the root one have an oversubscription factor of O,
meaning that each of those links has a bandwidth of S · B/O. The VOC model aims to relax the
dense connectivity requirement between all VMs, having oversubscription just for inter-group
communication.
30Root virtual switch
B·S/O
B·S/O
Group virtual
switch
VM 1
VM S
VM 1
VM S
Server
requirements
N VMs
Figure 3.22: A graphical representation of the VOC model
3.4.2.1
Usage
Oktopus [2]. Tenants who use a VOC model for their requests are aware of the bandwidth
oversubscription for inter-group communications as they explicitly have to specify the over-
subscription factor O in the request. Oktopus [2] though still does not make any distinction
between server resources, making it an unpractical solution for handling INP resources.
3.4.3
Tenant Application Graph (TAG)
As described in § 3.1.5.2, the Tenant Application Graph (TAG) model is a directed graph in
which vertexes represent server components and links’ weights represent the requested sending
and receiving bandwidth, respectively, SB and RB.
SB 12
RB 12
Server
comp. 1
SB 23
RB 23
Server
comp. 2
RB 21
SB 21
Server
comp. 3
Server composite
requirements
Figure 3.23: A TAG example
The model has been introduced and used by CloudMirror [16] since the previously-mentioned
models are inefficient as they over-allocate bandwidth (§ 3.1.5.1). This model is decoupled from
any network topology, allowing RMs to be more flexible during resource allocation.
3.4.3.1
Usage
CloudMirror [16]. The CloudMirror [16] placement algorithm just considers VM slots, as-
suming that all server resources have the same requirements. This is not true for INP ap-
plications since different INP resources require different network devices specifications. The
31placement algorithm tries to find lowest sub-tree in the physical topology that can host the
number of requested VMs taking into account the bandwidth requested between those: this
means that the scheduler is aware of the residual bandwidth on links, but it considers all net-
work devices to be the same.
3.4.4
Fine-grained resource requests
Most of state-of-the-art RMs [11, 28, 25, 21, 27] deal with resources in the simplest yet most
descriptive way: a list of server-only resources demands (e.g., CPU cores, memory, etc.).
3.4.4.1
Usage
Mesos [11]. First, the Mesos [11] logically centralized resource allocator offers different fine-
grained server resources to different schedulers. Schedulers will then reply to the allocator with
the information about the tasks to be run on servers. The allocator is completely network-
unaware and hence it cannot neither offer any other kind of resource types nor accept tasks to
be run on network devices.
Oktopus [2]. In Oktopus [2], tenant applications can use (i) the VC model to express virtual
networks characterized by a dense connectivity, (ii) the VOC model for oversubscribed virtual
networks and (iii) a fine-grained resource list to request server resources only. The system makes
a distinction between requests expressed by a virtual network (the first two options) and those
that are not (fine-grained list). The latter kind of requests has the lowest priority, meaning that
the bandwidth not used by virtual networks is equally distributed amongst applications that
did not request for any bandwidth guarantee. In conclusion, when sending a resource request
to Oktopus [2] expressed by means of a resource demand list, the RM will not take care of
bandwidth guarantees since there is no way for tenant applications to specify these demands.
Borg [28]. Borg [28] does not support any network abstraction and it only allows tenants
to request resources by listing them explicitly. A scheduler first step’s consists in finding the
subset of machines on which the task could run, and while doing this, it does not consider any
kind of aspect regarding the network. During the second step, namely scoring, which consists
in finding the best subset of machines on which the task can be allocated, the only network
aspect that is taken into consideration is the failure domain: tasks are allocated across failure
domains (e.g., machines, racks, power domains) for fault-tolerance.
Apache TM YARN [27]. Besides containers properties, tenant applications issuing requests
can include locality preferences (e.g., node-level, rack-level, and global locality preferences).
Exactly like Borg [28], this is the only network aspect taken into consideration during job
placement.
3.4.5
High-level goals
Rather than a fully descriptive resource model, high-level goals specification can be seen as
an add-on to the other previously-mentioned models that are worth mentioning for this thesis.
Taking Bazaar [12] as an example, one possible high-level goal is job completion time. Goals
like these eventually need to be translated into classic resource requirements to be correctly
interpreted by an RM.
32Chapter 4
Requirements
This chapters lists a set of functional (FR) and non-functional requirements (NFR) for the
resource model. Moreover, the model exposed to tenant applications (tenant-side model) and
the one internally used by the RM (RM-side model) could be different.
Functional requirements
FR1 The tenant-side model must be able to describe server and INP composites.
FR2 The tenant-side model must be able to describe all kinds of INP composites.
FR3 The RM-side model must be able to describe all kinds of INP composites.
FR4 The tenant-side model must allow the tenant to specify different bandwidth demands
for different composites and INP composites.
FR5 The resource model must be able to describe any kind of network topology.
FR6 The tenant-side model (and the corresponding APIs) must not change upon the release
of new INP solution or version.
FR7 The translation from composite requirements to logical resource requirements must be
done by the RM and not by the tenant application.
Non-functional requirements
NFR1 The tenant-side model should be able to easily express INP composite requirements
with the use of high-level laconic properties.
33Chapter 5
Design
This chapter lists all contributions this thesis has made, including (i) the design of a system
capable of handling composites, (ii) an appropriate resource model and (iii) an attempt of
generalization of the INP solutions previously described in § 3.2.
System design
This section contains a brief description of how the whole system works: from the tenant
resource request to the actual resource placement in the physical topology.
5.1.1
Overview
As required by FR1 and FR2, tenant applications must be able to express their requests
Mapping
composites to logical resources
in terms of logical resources and composites. The latter are just a simplification for tenant
applications, and they need to be translated into a set of logical resources for the placement
algorithm to allocate those. This translation can be done by the RM by means of a template
Composites
must
be translated
into into
logical
resources
database, that maps
pre-determined
composites
their equivalent
made out of just logical
resources, as shown in Figure 5.1.
• This is done by using a template database managed by the RM
composite {
type: inp,
app: incbricks,
chain_length: 3,
logical_switch {
app: incbricks,
Template
database
Query
issuer
IncBricks
Data
owner
Figure 5.1: Translating composites to logical resources
Logical resources represent the input of the placement algorithm. Those will be assigned to
physical resources, as shown in Figure 5.2.
34cal resources
logical_switch {
app: incbricks,
acement
hm places
resource
ysical ones
types of
al resources:
Placement
algorithm
er
ch
Rack
Figure 5.2: Placement of logical resources to physical resources
The whole picture
The whole system design is depicted in Figure 5.3.
composite {
type: inp,
app: incbricks,
chain_length: 3,
logical_switch {
app: incbricks,
Template
database
Placement
algorithm
Query
issuer
IncBricks
Data
owner
Rack
Logical resources &
composites
Logical resources only
Physical resources
Figure 5.3: From the tenant-side model to physical resources
5.1.2
Composites translation methods
Composites can have multiple properties specifying some application’s requirements or con-
strains.
A first approach called passive mapping would require the tenant application to explicitly
express internal composites’ properties that directly affect its equivalent expressed in terms
of just logical resources. Figure 5.4 shows an example of a tenant application explicitly speci-
fying the chain length of its IncBricks [17] composite and the bandwidth demands B 1 and B 2
towards and outwards it, respectively. This of course increases the expressiveness of the tenant
application with the cost of making the interface more complex to use 7 NFR1.
With the opposite approach (active mapping), tenant applications do not have to specify
internal composites’ properties, but instead more abstract performance goals. These high-level
composites’ goals will be then translated by the RM. An example of this translation is showed
35User specified
1 st alternative: passive template mapping
composite {
type: inp,
app: incbricks,
chain_length: 3,
logical_switch {
app: incbricks,
Directly influences
Template
database
Query
issuer
IncBricks
Data
owner
edge {
bandwidth: B 1 ,
edge {
src: ...,
dst: ...,
bandwidth: B 2 ,
Logical resources &
composites
Logical resources only
Figure 5.4: Passive template mapping
in Figure 5.4, where the requested tuple rate is transformed accordingly into topology and
bandwidth constraints. This approach simplifies the interface exposed to tenant applications
by not letting them taking care of internal composites properties that might be unknown to
nd 3 NFR1.
developers
composite {
type: inp,
app: incbricks,
2 alternative: active template mapping
The RM computes
more specific
requirements
logical_switch {
app: incbricks,
3 switches
Template
database
Query
issuer
IncBricks
Data
owner
User specified
edge {
tuple_rate: B 1 ,
Blue quantities are
derived by the RM
Logical resources &
composites
Logical resources only
Figure 5.5: Active template mapping
Resource model design
Tenant applications need a tenant-side model to be able to include INP resources in their
requests. This chapter will first discuss the integration of INP resources in a RM using currently-
existing models described in § 3.4 and will then introduce a model proposal which must be
capable of describing existing INP solutions and possibly even future ones.
365.2.1
Integrating INP resources using existing resource models
Different resource models (§ 3.4) are currently used by RMs to offer and manage data center
resources. Most of the times they allow the description of logical server resources only, but some
Resource Management Frameworks (RMFs) like [8, 16, 2] also provide bandwidth provisioning
by allowing tenants to request logical edge resources. A lot of them are network-aware in the
sense of being aware of failure domains. [20] allows tenants to request logical switch resources
but it does not allow them to request logical edge resources and composites.
This section will discuss whether each resource model previously described in § 3.4 can be
used for this system or not, confronting them with the requirement list reported in chapter 4.
5.2.1.1
Virtual Cluster (VC)
Quick recap. The Virtual Cluster (VC) model consists in N VMs connected to a single
virtual switch by a link of bandwidth B. Figure 5.6 shows an example.
Virtual switch
VM 1
VM N
Server
requirements
Figure 5.6: A graphical representation of the VC model
Requirements check. Expressing just one value of bandwidth B for all connections is indeed
a big limitation. One could assign to B the highest bandwidth demand in case different nodes
require different amounts of bandwidth, but this is obviously a waste of resources. This does
not completely satisfy 7 FR4. The Time-Interleaved Virtual Cluster (TIVC) model introduced
in Proteus [30] overcomes this limitation since bandwidth constraints are expressed by time-
varying functions instead of a constant fixed value. Both models though (i) rely on a single
virtual switch that is not suitable for tenant applications which require a more complex logical
switch topology such as a chain or a tree (7 FR5) and (ii) do not support logical switch resource
requests (7 FR2).
5.2.1.2
Virtual Oversubscribed Cluster (VOC)
Quick recap. The VOC model is the oversubscribed extension of the VC one. Figure 5.7
shows an example.
Requirements check. The VOC model has the advantage of not requiring a virtual switch
of bandwidth N · B, that could be unpractical when N is particularly large. When a tenant
application issues a resource request using a VOC, it requests for a < N, B, S, O > tuple, being
respectively (i) the number of VMs, (ii) the bandwidth needed for intra-group communication,
(iii) the group size and (iv) the oversubscription factor. Still, certain kinds of applications might
need different values of B for different groups (7 FR4). Furthermore, the two-layer logical tree
topology might not suit perfectly for applications that require a chain having more than 2
37Root virtual switch
B·S/O
B·S/O
Group virtual
switch
VM 1
VM S
VM 1
VM S
Server
requirements
N VMs
Figure 5.7: A graphical representation of the VOC model
network devices or a tree topology with more than 2 layers (7 FR5). The VOC model still does
not support logical switch resource requests (7 FR2).
A possible variant. The two limitations previously mentioned can be obviously overcome
by allowing tenants to specify (i) logical switch resources (satisfying 3 FR2) (ii) different
bandwidth demands for different groups (satisfying 3 FR4) and (iii) an arbitrary tree height.
The corresponding variant is shown in Figure 5.8.
Switch
requirements
VM 1
B N/S
VM S
B N/S
VM 1
VM S
Server
requirements
VM 1
VM S
N VMs
Figure 5.8: A possible VOC variant with an arbitrary tree height
However, tenant applications still (i) cannot express any kind of switch topology (e.g., a
switch loop for an in-network Distributed Hash Table (DHT) chord), violating 7 FR5, and
(ii) convert INP high-level requirements (e.g., in-network total cache size, lock requests per
second, etc.) into switch resource requirements (7 FR7).
385.2.1.3
Tenant Application Graph (TAG)
Quick recap. A TAG is a graph where vertexes represent server composites, connected by
logical edge resources. Figure 5.9 shows an example.
SB 12
RB 12
Server
comp. 1
SB 23
RB 23
Server
comp. 2
RB 21
Server
comp. 3
SB 21
Server composite
requirements
Figure 5.9: A TAG example
Requirements check. The model does not allow tenant applications to request INP com-
posites (7 FR1) and logical switch resources (7 FR2). Since this model satisfies all other
requirements, it will be used as a starting point for the resource model proposed in § 5.2.2.
5.2.2
Model proposal: the extended-Tenant Application Graph
The extended-Tenant Application Graph (eTAG) is based on a TAG and it allows tenant applica-
tions to specify (i) all types of composites (3 FR1) and logical resources (3 FR2), (ii) different
bandwidth demands (3 FR4) for different entities and (iii) any kind of network topology (3
FR5). An example of an eTAG is depicted in Figure 5.10.
SB 12
RB 12
Server
comp. 1
SB 23
comp. 1
RB 21
Server composite
requirements
SB 21
RB 23
Server
logical
res. 3
INP composite
requirements
Server logical resource
requirements
Figure 5.10: The eTAG proposed model
The introduction of composites in the model delegates their translation to logical resources
to the RM and not by the tenant application (3 FR7). The template database (§ 5.1.1) allows
tenants to express INP composite requirements in terms of high-level properties (3 NFR1).
Generic groups
The template database (§ 5.1.1) takes care of translating composites into a set of logical resources
in order for the placement algorithm to work (Figure 5.1). Ideally, the template database should
map individual INP solutions to their equivalent made out of logical resources only, but this
may be not feasible as the number of INP solutions may increase in the future. That is to say,
this approach is not scalable.
39INP solutions may generally seem very different from each other as they pursue different
goals, but grouping them based on common aspects might be a way to decrease the number of
entries of a template database. That said, there is no unique way of categorizing INP solutions:
for instance, one could group them based on their final purpose, on their logical architecture,
and so on. This section tries to do this based on the latter aspect, using those INP solutions
described in § 3.2.
5.3.1
In-network data aggregation
Two key examples of INP solutions that might belong to such a group are Daiet [24] and
SHArP [10]. Usually network devices must (i) form a tree whose root is connected to data
consumers and whose leaves are connected to data producers (ii) dedicate part of their local
memory to store a key-value map (iii) be able to perform basic operations on data, such as
writing and hashing (iv) wait for all its children to send aggregated data. Then of course single
INP solutions may vary for different aspects: for instance, SHArP [10] supports multiple data
consumers, while Daiet [24] expects only one.
Data producers must be connected to exactly one tree leaf, and data consumer(s) must be
connected to the tree root.
Daiet [24] and SHArP [10] differ a lot when it comes to nodes management. In Daiet [24],
the SDN controller must push flow rules to all switches belonging to at least one tree. On the
other hand, SHArP [10] has a special unit (not necessarily be the SDN controller) that acts like
a sort of RM, dedicating SHArP [10] resources to those entities who request for them.
5.3.2
In-network storage
Both NetChain [13] and IncBricks [17] make use of an distributed key-value map across phys-
ical switches, even if for a different purpose. NetChain [13] in fact implements an in-network
coordination service, while IncBricks [17] simply caches data.
Network devices must dedicate part of their local memory to store a distributed map. They
also must form a chain: in case of IncBricks [17], the two communication endpoints must be
connected through this chain, while in NetChain [13] this is not a requirement since coordination
can involve more than two nodes (but switches must support coordination primitives).
A query issuer is a logical server resource and must know how to contact at least the head
switch. In NetChain [13], query issuers might also directly contact the tail switch, and they
must include the list of IP addresses of all switches belonging to the chain in their packets.
IncBricks [17] requires a logical server resource to own the cached data, and it (i) must be
connected to the tail switch and (ii) can reply to queries in case no switch has cached the data.
The SDN controller is always in charge of forming the switch chain.
40Chapter 6
Evaluation
Goal
This chapter aims to evaluate the system designed in chapter 5. More specifically, the ultimate
goal is to show new challenges and problems brought by the introduction of INP resources and
composites into the resource management layer.
Ultimately, it is expected that these two kinds of physical resources (server and switch
physical resources) can act as a bottleneck for each other, as physical switch resources are very
limited compared to server ones (§ 6.2.1).
Simulator
A simulator very similar to the Omega’s [25] lightweight simulator † has been built up from
scratch for this evaluation in Scala so it could support multiple resource dimensions, physical
switch resources, composites and more. Its key aspects are described below.
6.2.1
Physical resources
Data center architecture. This simulator emulates physical switch resources besides server
ones. The simulator can support any kind of data center network architecture, but a fat-tree
has been used for this evaluation, which is a very common topology for data centers to use. An
example of a fat-tree topology is depicted in Figure 6.1.
This fat-tree has three layers of switches: core, aggregation and a last one which is usually
called edge, layer, access or just simply ToR switches. Being k the number of pods in the
topology, a fat-tree contains (k/2) 2 core switches, k 2 /2 aggregation switches, k 2 /2 ToR switches,
and supports up to k 3 /4 servers.
Physical switch resources. For the sake of simplicity, physical switches have a single nu-
merical dimension called Compute Unit (CU). This assumption does not affect this evaluation’s
results and it makes the scheduling algorithm a bit simpler. Increasing the number of switch di-
mensions is trivial since the simulator already supports multiple dimensions for servers, namely
CPU and memory. Logical edge resources have been ignored to simplify the placement algo-
rithm.
Switches are also characterized by a property map, that ultimately suggests which kinds of
INP solutions they’re able to run. In this evaluation, properties and INP solutions coincide (e.g.,
Available at github.com/google/cluster-scheduler-simulator
41Core layer
Aggregation
layer
ToR switches
Figure 6.1: The fat-tree topology
switch A supports INP solutions X, Y and Z), but these properties might be more appropriately
extended to any other kind of hardware property that distinguish that switch in the data center
(e.g., CPU architecture, supported data plane programming language, etc.). In general, a switch
task requesting for property P , will only be allocated on a switch if it supports that property
6.2.2
Workload
The lack of workloads containing INP requests (and furthermore, of composites) represents
indeed a big limitation. To keep things easy, INP requests are randomly generated according to
the generic groups described in § 5.3. The percentage of INP request injected in the workload
is kept as a parameter sweep, so it is possible to observe how simulations differ with different
amount of INP requests.
Tweaks. Many factors come into play when generating INP requests from classic servers-only
workloads. One key aspect is the reduction of server tasks once an INP solution is introduced:
it will be called Server Tasks Cutback (STC). A formal definition could be the following:
ST C =
#server tasks without IN P
#server tasks with IN P
(6.1)
Undoubtedly, the STC value will heavily affect simulations and its value depends on the
specific INP solution being used. For simplicity, the STC value will be fixed to 5. In real
environments, tenant applications will directly include INP solutions in their request. The STC
might only be useful in case the scheduler is smart enough to propose alternatives to servers-
only requests, in which case it will have to compute its value based on all the INP solutions
included in the offer as well as other factors.
Another aspect to take into consideration is the number of switch tasks to generate when
creating INP requests. This also depends on the specific INP solutions being used, but for
simplicity, is it logarithmically proportional to the number of server tasks to which it is directly
connected to in the eTAG.
Template database. Some of the servers-only requests are enhanced with INP composites.
More specifically, logical server resource requirements will see their number of server tasks
42reduced by STC. Then, a simple INP composite will be inserted in the eTAG and connected to
these server task groups. According to the INP solutions analysis § 3.2, NetChain [13] and Daiet
[24] composites expect a single ”data producer and consumer” node, while IncBricks [17] and
SHArP [10] composites stay in between two distinct producer and consumer nodes. Therefore,
the simulator splits the server task group in two in case the randomly-generated INP solution
belongs to the latter group.
6.2.3
Scheduler
The scheduler used for this evaluation uses a simple greedy placement algorithm that cycles
through all physical resources every time a job needs to be scheduled. It tries to allocate
as many tasks as possible on the same physical machine simply by (i) checking the amount
of available numerical resources and (ii) performing the properties check for switch tasks as
described in § 6.2.1.
Testbed
Experiments ran on Dell C6420 servers on CloudLab [22], simulating a 1 day long workload.
6.3.1
Metrics
These simulations focus on the average resource utilization in the data center for all dimensions.
Different simulations vary in the ratio between tenant requests including INP composites over
those requests including logical server resources only.
Results
Figure 6.2 shows moving averages of physical resource utilization across all dimensions of four
different simulations in which the ratio of INP requests over classic servers-only requests vary
from 0% up to 100%. Predictably, the more INP requests tenants ask for, the more Compute
Units (CUs) will be used. Depending on the Server Tasks Cutback (STC) value (§ 6.2.2) and
by the amount of switch tasks required by different INP solutions, server and switch utilization
may drop/increase at different rates.
Figure 6.3 shows this very same behavior in a different way, plotting the average (during
the whole simulation) resource utilization across all dimensions as a function of the amount of
INP requests.
When plotting the less relatively-used resource dimension as in Figure 6.4, a peak around a
certain percentage of INP requests appears.
43100%
0.0% of INP requests
33.3% of INP requests
Utilization
40% Resource utilization
Switch CU
Server CPU
Server memory
0%
100%
66.7% of INP requests
100.0% of INP requests
0%
00h 02h
Simulation time
Simulation time
Figure 6.2: Physical resource utilizations for different amounts of INP requests
100%
Resource utilization
Avg. switch CU
Avg. server CPU
Avg. server memory
0%
0%
INP requests ratio
100%
Figure 6.3: Average resource utilization as a function of the INP requests ratio
44100%
0%
0%
INP requests ratio
100%
Figure 6.4: Minimum resource utilization across all dimensions
45Chapter 7
Conclusions
Based on the results achieved in § 6.4 and on the analysis of network-aware RMs in § 3.3.2,
this chapter aims to foresee some features that an ideal fully-INP aware should have, as well as
some open problems in the INP resource management field.
Fully INP-aware RM features
Conjunct placement. As mentioned in § 3.3.2, [20] considers both server and logical switch
resources during allocation, but it does not place them during the same scheduling round.
The drawback of not scheduling these two kinds of resources conjunctly bring the placement
algorithm to derive a sub-optimal placement, as explained in § 3.3.2.
Ideally, a RM should allocate a job considering its server and logical switch resources at the
same time, i.e., during the same placement round.
INP alternatives. The results reported in § 6.4 show an underutilization of physical resources
depending on the amount of tenant requests containing INP composites.
To maximize the relative minimum physical resource utilization, the RM would need to
adjust the ratio of INP requests over servers-only ones. It is expected that physical switch
resources will become the bottleneck of the system more easily, due to their limited amount
compared to server ones. For this reason, RMs should be able to propose alternatives to INP
composites made out of logical server resources or server composites only, as soon as physical
switch resources become heavily utilized. Those alternatives could be stored, for instance, in the
same template database (§ 5.1.1) used to translate INP composites into a set of logical switch
resources.
Open problems
Some of the important metrics used in chapter 6 are still unknown due to the lack of operational
RMs that handle INP requests. In order for the system designed in chapter 5 to work, the
performance improvement of all INP solutions in the template database must be reduced to a
single number that represents the reduction of server tasks after the introduction of the INP
composite: the Server Tasks Cutback (STC).
Moreover, there should be a precise way of determining the amount of switch tasks needed
to implement an INP composite, and this could depend on a lot of factors like the set nodes to
which the INP composite is connected to in the eTAG, its high-level properties, etc.
46Another key aspect to consider is the type of life cycle that different INP solutions might
have. Similarly to server requests, batch (short-term) and service (long-term) jobs may need
different scheduling policies. INP solutions should also be categorized on the same basis, e.g.,
NetChain [13] as a service INP composite (since most likely it will be running for a long time,
serving multiple server tasks) and Daiet [24] or SHArP [10] as batch.
47Abbreviations
AM Aggregation Manager. 23, 24, 25
AN Aggregation Node. 23, 24, 25
API Application Programming Interface. 3, 4, 23, 29, 33, 50
CU Compute Unit. 41, 43
DHT Distributed Hash Table. 38
eTAG extended-Tenant Application Graph. 39, 42, 43, 46
HPC High Performance Computing. 23
INP In-Network Processing. 3, 4, 5, 7, 8, 13, 26, 27, 29, 30, 31, 33, 34, 36, 38, 39, 40, 41, 42,
43, 44, 46, 47, 50
MPI Message Passing Interface.
NBI SDN Northbound Interface.
NFV Network Function Virtualization. 7, 8
QP Queue Pair.
RM Resource Manager. 3, 4, 5, 6, 7, 8, 10, 11, 13, 25, 26, 27, 31, 32, 33, 34, 35, 36, 37, 39, 40,
46, 50
RMF Resource Management Framework. 37
RPC Remote Procedure Call. 8
RTT Round Trip Time.
SBI SDN Southbound Interface. 7
SDN Software Defined Networking. 7, 8, 13, 17, 18, 21, 22, 40
SFC Service Function Chaining.
STC Server Tasks Cutback. 42, 43, 46
48TAG Tenant Application Graph. 4, 12, 27, 31, 39
TCA Target Channel Adapter.
TIVC Time-Interleaved Virtual Cluster. 27, 37
ToR Top of Rack. 5, 41
VC Virtual Cluster. 27, 29, 30, 32, 37
VM Virtual Machine. 5, 7, 9, 11, 12, 22, 27, 28, 29, 30, 31, 32, 37, 50
VNF Virtual Network Function. 7
VOC Virtual Oversubscribed Cluster. 11, 27, 30, 31, 32, 37, 38
49Resources glossary
Physical resource physical hardware component of limited availability within a physical ma-
chine. 4, 12, 27, 34, 35, 41, 43, 44, 45, 46
Physical server resource resource of a physical server machine. 26, 27, 41, 46,
Physical switch resource resource of a physical switch, network accelerator, middle-box
and of every kind of network device originally intended to forward packets. 27, 40, 41, 42, 46
Logical resource logical representation of a physical resource. 12, 13, 33, 34, 35, 39
Logical server resource virtualized server physical resource, often implemented by means
of a VM, container or an entire physical server. 4, 26, 27, 37, 40, 42, 43, 46, 47
Logical switch resource logical representation of a physical switch resource not mapped to
any physical switch device. 26, 27, 37, 38, 39, 43, 46
Logical edge resource properties of virtual connections between two logical resources, e.g.,
bandwidth, latency, etc. 26, 27, 37, 39, 41
Composite template describing a high-level logical component. It can be made out of other
composites and/or logical resources.. 26, 33, 34, 35, 36, 37, 39, 41, 42, 43
Server composite composite describing a high-level server component, e.g., web server,
databases, etc. 39, 46
INP composite composite describing a high-level INP application, e.g., IncBricks [17],
NetChain [13], etc. 33, 39, 42, 43, 46, 47
Resource model model capable of describing composites and logical resources. The model
exposed to tenants and the one internally used by the RM could be different. 11, 33, 34, 37, 39
Tenant-side model resource model exposed to tenants by the system API. 27, 33, 35, 36
RM-side model resource model internally used by the placement algorithm in order to
allocate logical resources.