Existing \glspl{rm} have a different awareness level of switch and link resources: based on this, \glspl{rm} can be divided into three groups.

\paragraph{\glspl{vm} proximity-aware}
Most of the \glspl{rm} out there can spread \glspl{vm} across different failure domains like machines, racks and power domains.
This group is not worth discussing since these kinds of \glspl{rm} do not consider any kind of resources rather than server ones.
Some examples of \glspl{vm} proximity-aware \glspl{rm} are Omega \cite{omega}, \glsdesc{yarn} and Mesos \cite{mesos}.

\paragraph{Bandwidth-aware}
\glsreset{vc} \glsreset{tivc} \glsreset{voc} \glsreset{tag}
Some \glspl{rm} like CloudMirror \cite{cloudmirror}, Oktopus \cite{oktopus}, Kraken \cite{kraken} and Proteus \cite{proteus} allow tenants to specify bandwidth demands.
These \glspl{rm} let tenants express their requests by using "virtual network" \glslink{model:tenant}{models} like \glspl{vc}, \glspl{tivc}, \glspl{voc} and \glspl{tag}.

Oktopus \cite{oktopus} and Kraken \cite{kraken} assume that every \gls{vm} can be placed on every physical server (i.e., \textit{\gls{vm} slots} instead of dealing with \glslink{resource:physical}{resource} dimensions), completely ignoring server-local resource requirements.
This is not acceptable in a real-world scenario in which different \glspl{resource:logical:server} have different \glslink{resource:physical}{resource} requirements.

It is also worth to notice that Kraken \cite{kraken} allows tenants to \textit{upgrade} their bandwidth requirements, causing the migration of \glspl{vm} that have been previously allocated in parts of the data center in which the new bandwidth requirements could not be satisfied anymore.
Still, none of these \glspl{rm} is able to manage any kind of switch resources.

\paragraph{Network resources-aware} \label{network_resources-aware_rms}
The most interesting group consists of those \glspl{rm} that are actually aware of \glslink{resource:physical:switch}{network resources}.
At the time of writing, there seems to be only one embedding solution that belongs to this group, namely, the one introduced in "On tackling virtual data center embedding problem" \cite{ontackling} by Rabbani, Md Golam, et al. presented in the \textit{IM 2013: IFIP/IEEE International Symposium on Integrated Network Management} conference.
This solution allows tenants to explicitly specify
\begin{mylist}
    \item \glspl{resource:logical:server},
    \item \glspl{resource:logical:switch}, and
    \item bandwidth demands as \glspl{resource:logical:edge}
\end{mylist}.

Tenants use a graph to express their resource requests.
The graph is expressed as a key-value map that includes
\begin{mylist}
    \item a set of \glslink{resource:logical:server}{\glsentryshort{vm} resources},
    \item a set of \glspl{resource:logical:switch} and
    \item a set of \glslink{resource:logical:edge}{logical links} connecting the above entities (and their minimum required bandwidth)
\end{mylist}.

Its placement algorithm is interesting for the scope of this thesis since it is the only one that takes into account all three types of resources mentioned before.
The placement phase is divided into three parts:
\begin{mylist}
    \item the \glspl{vm} placement,
    \item the \glslink{resource:logical:switch}{logical switches} placement, and
    \item the \glslink{resource:logical:edge}{logical links} placement
\end{mylist}.
The problem of placing \glspl{vm} is reduced to a min-cost flow one like showed in \autoref{ontacklingfirststep}.

\begin{figure}[!htb]
    \centering
    \usebox{\ontacklingfirststep}
    \caption{\gls{vm} placement in \cite{ontackling}}
    \label{ontacklingfirststep}
\end{figure}

The graph shown in \autoref{ontacklingfirststep} is built in the following way: \glspl{vm} are sorted according to a requested resource capacity in descending order and placed on the left side of the image.
Physical servers are instead placed on the right side.
\glspl{vm} are connected to physical servers only if they can be placed on them.
A dummy source and destination node are then added like shown in \autoref{ontacklingfirststep} so that an instance of a min-cost flow solver can be run on it.
The outcome of this phase provides the allocation of \glspl{vm} on physical servers.

It is important to notice how \glspl{vm} are sorted based on just one resource dimension.
Even though the model used in \cite{ontackling} supports an infinite number of resource dimensions, the placement algorithm only supports one dimension and it does not seem to exist an easy way to extend it to support multiple dimensions.
The same thing is done for the placement of logical switches, as shown in \autoref{ontacklingsecondstep}.

\begin{figure}[!htb]
    \centering
    \usebox{\ontacklingsecondstep}
    \caption{Logical switches placement in \cite{ontackling}}
    \label{ontacklingsecondstep}
\end{figure}

The mapping of logical switches to physical ones is done independently of the outcome of the previous phase (i.e., \gls{vm} placement).
\autoref{ontacklinginefficientswitchplacement} shows how this could lead to a bandwidth waste in case a logical switch connecting two \glspl{vm} is mapped to a physical switch that is far away from the physical servers on which the \glspl{vm} have been previously placed.

\begin{figure}[!htb]
    \centering
    \usebox{\ontacklinginefficientswitchplacement}
    \caption{An example of an inefficient logical switch placement in \cite{ontackling}}
    \label{ontacklinginefficientswitchplacement}
\end{figure}

The third and last step simply consists of mapping logical links between two logical entities (\glspl{vm} or switches) to the shortest physical path connecting the physical devices on which the logical entities have been allocated.

The placement algorithm is not fault-tolerant since it tries to map as many \glspl{vm} as possible to the same physical server to minimize server resource fragmentation.